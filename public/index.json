[{"body":"","link":"https://www.thojotech.com/","section":"","tags":null,"title":""},{"body":"","link":"https://www.thojotech.com/tags/azure/","section":"tags","tags":null,"title":"Azure"},{"body":"","link":"https://www.thojotech.com/categories/azure/","section":"categories","tags":null,"title":"Azure"},{"body":"","link":"https://www.thojotech.com/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://www.thojotech.com/tags/github/","section":"tags","tags":null,"title":"GitHub"},{"body":"","link":"https://www.thojotech.com/tags/hugo/","section":"tags","tags":null,"title":"Hugo"},{"body":"","link":"https://www.thojotech.com/categories/hugo/","section":"categories","tags":null,"title":"Hugo"},{"body":"","link":"https://www.thojotech.com/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://www.thojotech.com/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"This article is based on Microsoft’s documentation -- Use GitHub Actions workflow to deploy your static website in Azure Storage --, but has been updated to reflect changes required to publish Hugo static sites to Azure blob storage/static web sites. Please see the article linked above for a list of prerequisites.\nGenerate deployment credentials Using Azure CLI\n1$azsubId=\u0026#34;xxxxxxxxxxxxxxxxxxxxxx\u0026#34; 2$azStrAcctResourceGroup=\u0026#34;rg-westus2-website\u0026#34; 3$azstorageacct=\u0026#34;stgwestus2webstorageacct\u0026#34; 4az ad sp create-for-rbac --name $azstorageacct --role contributor --scopes /subscriptions/$azsubId/resourceGroups/$azStrAcctResourceGroup --sdk-auth The command will generate a JSON object, copy and save for later:\n1{ 2 \u0026#34;clientId\u0026#34;: \u0026#34;\u0026lt;GUID\u0026gt;\u0026#34;, 3 \u0026#34;clientSecret\u0026#34;: \u0026#34;\u0026lt;GUID\u0026gt;\u0026#34;, 4 \u0026#34;subscriptionId\u0026#34;: \u0026#34;\u0026lt;GUID\u0026gt;\u0026#34;, 5 \u0026#34;tenantId\u0026#34;: \u0026#34;\u0026lt;GUID\u0026gt;\u0026#34;, 6 \u0026#34;activeDirectoryEndpointUrl\u0026#34;: \u0026#34;https://login.microsoftonline.com\u0026#34;, 7 \u0026#34;resourceManagerEndpointUrl\u0026#34;: \u0026#34;https://management.azure.com/\u0026#34;, 8 \u0026#34;activeDirectoryGraphResourceId\u0026#34;: \u0026#34;https://graph.windows.net/\u0026#34;, 9 \u0026#34;sqlManagementEndpointUrl\u0026#34;: \u0026#34;https://management.core.windows.net:8443/\u0026#34;, 10 \u0026#34;galleryEndpointUrl\u0026#34;: \u0026#34;https://gallery.azure.com/\u0026#34;, 11 \u0026#34;managementEndpointUrl\u0026#34;: \u0026#34;https://management.core.windows.net/\u0026#34; 12 } Configure the GitHub secret In GitHub, browse our repository. Select Settings \u0026gt; Secrets \u0026gt; New secret. Paste the entire JSON output captured earlier. For secret name, use AZURE_CREDENTIALS When you configure the workflow file later, you use the secret name for the input creds of the Azure Login action, for example: 1- uses: azure/login@v1 2with: 3 creds: ${{ secrets.AZURE_CREDENTIALS }} Add your workflow Go to Actions for your GitHub repository. Select Set up your workflow yourself. Delete everything from the workflow and replace with the following code block: 1# This is a basic workflow to help you get started with Actions 2 3name: Blob storage website CI 4 5# Controls when the workflow will run 6on: 7 # Triggers the workflow on push or pull request events but only for the \u0026#34;main\u0026#34; branch 8 push: 9 branches: [ \u0026#34;main\u0026#34; ] 10 11jobs: 12 build: 13 runs-on: ubuntu-latest 14 steps: 15 - uses: actions/checkout@v2 16 - uses: azure/login@v1 17 with: 18 creds: ${{ secrets.AZURE_CREDENTIALS }} 19 - name: Upload to blob storage 20 uses: azure/CLI@v1 21 with: 22 inlineScript: | 23 az storage blob upload-batch --account-name yourstaticwebsitestorageaccount --auth-mode key -d \u0026#39;$web\u0026#39; -s ./public --overwrite 24 - name: Purge CDN endpoint 25 uses: azure/CLI@v1 26 with: 27 inlineScript: | 28 az cdn endpoint purge --content-paths \u0026#34;/*\u0026#34; --profile-name \u0026#34;thojotechwebcdn\u0026#34; --name \u0026#34;thojotechwebcdn\u0026#34; --resource-group \u0026#34;rg-eastus-thojotech-01\u0026#34; --no-wait 29 30# Azure logout 31 - name: logout 32 run: | 33 az logout 34 if: always() In the example above, notice the changes required for publishing Hugo static content, which uses a public folder to store the static content. That is the folder we need to copy to Azure Blob storage.\nNow, when you commit changes to your GitHub static website repository, the public folder will be copied to your Azure static web site blob storage automagically.\n","link":"https://www.thojotech.com/post/hugoazuregithub/","section":"post","tags":["Azure","Hugo","GitHub"],"title":"Use GitHub Actions workflow to deploy your Hugo static website in Azure Storage"},{"body":"I recently started scripting with Azure CLI within my Windows Subsystem for Linux (WSL) environment and tried to create a database on an Azure Database for MySQL Flexible-Server instance and received an error 'flexible-server' is not in the 'az mysql' command group. I also tried to list out the flexible-servers in my subscription, but it also give the same error!\nI tried to see if maybe my Azure CLI was out of date, but it was already at the newest version! As I test, I tried running the command in Microsoft's pre-package Azure CLI Docker container within my WSL Ubuntu environment: (You will need Docker Desktop installed)\n~$ docker run -it mcr.microsoft.com/azure-cli\nSure enough, it worked just fine! Something is wrong with my installation of Azure CLI within the WSL environment. It is a relatively new install, so possibly Azure CLI did not install completely when setting up WSL…\nNot matter, I found the instructions for reinstalling Azure CLI here… Install the Azure CLI on Linux\nGet packages needed for the install process: 1~$ sudo apt-get update 2~$ sudo apt-get install ca-certificates curl apt-transport-https lsb-release gnupg Download and install the Microsoft signing key: 1~$ curl -sL https://packages.microsoft.com/keys/microsoft.asc | 2 gpg --dearmor | 3~$ sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg \u0026gt; /dev/null Add the Azure CLI software repository: 1~$ AZ_REPO=$(lsb_release -cs) 2~$ echo\u0026#34;deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPOmain\u0026#34;| 3 sudo tee /etc/apt/sources.list.d/azure-cli.list Note: This step did not work as expected in the WSL Ubuntu environment, lsb_release doesn't return any information, so you have to set AZ_REPO environment variable manually:\nExecute \u0026quot;cat /etc/*release\u0026quot; Look for VERSION_CODENAME, or UBUNTU_CODENAME, which in our case is focal Now you can continue with the rest of the command: 1~$ AZ_REPO=focal 2~$ echo\u0026#34;deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPOmain\u0026#34;| 3 sudo tee /etc/apt/sources.list.d/azure-cli.list Update repository information and install the azure-cli package: 1~$ sudo apt-get update 2~$ sudo apt-get install azure-cli The az mysql flexible-server module did not light up right away, I had to restart my Ubuntu WSL session. After that, it works fine! ","link":"https://www.thojotech.com/post/azmysqlflexclierr/","section":"post","tags":["Azure","PowerShell","Cli","MySql","WSL","Ubuntu"],"title":"Az MySql Flexible-server command returns error"},{"body":"","link":"https://www.thojotech.com/tags/cli/","section":"tags","tags":null,"title":"Cli"},{"body":"","link":"https://www.thojotech.com/tags/mysql/","section":"tags","tags":null,"title":"MySql"},{"body":"","link":"https://www.thojotech.com/tags/powershell/","section":"tags","tags":null,"title":"PowerShell"},{"body":"","link":"https://www.thojotech.com/tags/ubuntu/","section":"tags","tags":null,"title":"Ubuntu"},{"body":"","link":"https://www.thojotech.com/tags/wsl/","section":"tags","tags":null,"title":"WSL"},{"body":"","link":"https://www.thojotech.com/tags/box/","section":"tags","tags":null,"title":"Box"},{"body":"","link":"https://www.thojotech.com/tags/microsoft/","section":"tags","tags":null,"title":"Microsoft"},{"body":"","link":"https://www.thojotech.com/categories/notes-from-the-field/","section":"categories","tags":null,"title":"Notes from the field"},{"body":"On a recent Microsoft Office365/Box to OneDrive migration project, I was asked to put together a lessons-learned for the project. Listed below are some of those lessons. The Microsoft Office 365 upgrade project had in scope upgrading users from Office 2010 to O365.\nMy role: Technical Advisor Company type: Large Hospitality \u0026amp; Gaming\nMake sure you have a good sampling of users in the pilot. Office workers, executives, etc.\nInventory is everything.\nReally need good configuration management to help determine where computers reside. --COMPANY-- network team does not keep up to date records of what subnets exist at the properties. Workday, their employee management/organizational tool was not always accurate in determining where a user resides. Users moved around a lot and paperwork was slow to follow them.\nUDA, (user device affinity) in Microsoft System Center Configuration Manager, is challenging in multiuser environments. In some cases, 5-10 users could share one computer, or 30-40 users could share 20 or more computers\nDon’t wait until the project is inflight to begin compatibility testing\nIf you are deploying to multiple locations/properties, you need a point person(s) and PM per property. The Senior PM will be overwhelmed with scheduling if they are trying to handle it for all properties, you need multiple PMs assigned\nWhat we found out is --COMPANY-- runs their business on PSTs. Due to the limited mailbox size allocated to users 500MB or less, users save ALL their important mail to PST’s. Microsoft BROKE the functionality for PST’s beginning with Outlook 2013. Breaking changes include the inability to reference PST’s on Network drives over 4GB in size and inability to SEARCH PST’s located on network drives. Other than moving the PST data to the O365 Online Archive, still do not have a good answer how different departments should be storing information from emails. If you have a short email retention policy, where do you store this data, which might include contracts, agreements, etc, that should persist beyond most email retention policies.\nLegal considerations might prevent you from migrating users email from onprem to O365, make sure you understand what your limitations are prior to the project beginning and understanding how these situations should be handled. More than likely, you will need additional documentation and \u0026quot;evidence\u0026quot; regarding fidelity of email migrations (Exchange on-premises to Exchange Online/O365), and file migrations (Box to OneDrive).\nShared calendars, mailboxes and other resources present a challenge when it comes to migration from on-prem to O365. You cannot have split homes for shared resources. If you have multiple people sharing resources, they must be cutover at the same time, or those left behind On-prem will not be able to manage those resources.\nThird party SSO really complicates everything. Introduces problems in syncing licensing.\nMake sure you have a good mobile device management strategy. --COMPANY-- uses OKTA but has no way to deploy to iPhones or Android devices, email cutover was made more difficult due to having to provide resources to help users change configurations on their phones.\nMake sure other projects like desktop refresh do not coexist at the same time. In many cases, hardware refresh would complete a week or less before we upgraded a property and it wrecked UDA and we were unable to target users with confidence.\nBox to OneDrive Migration Make sure you identify users who may have legal restrictions. --COMPANY-- users on legal hold, I had to document and submit a report on all the files they owned and that they were copied to OneDrive without any modifications.\nYou should probably archive ALL data from the source in case retrieval is required. We had quite a few cases of users leaving the company, but their files on Box were still shared. We did not move users unless they were active in the company.\nPicking a good tool like SkySync allowed the project to move faster.\nOneDrive had more file length and path limitations that Box, so reports had to be run to determine where these files resided and moved manually.\nOneDrive is slower than Box, if you need faster access, make sure you negotiate this with Microsoft ahead of time, or use Azure blob storage for things like digital media\n","link":"https://www.thojotech.com/post/notes-o365/","section":"post","tags":["Office365","Box","OneDrive","Microsoft"],"title":"Notes from the field - Microsoft Office 365 Migration"},{"body":"","link":"https://www.thojotech.com/tags/office365/","section":"tags","tags":null,"title":"Office365"},{"body":"","link":"https://www.thojotech.com/tags/onedrive/","section":"tags","tags":null,"title":"OneDrive"},{"body":"Blog with an emphasis on technology, particularly as it relates to Microsoft and Microsoft Azure and other random things.\nJoe has over 20 years of experience in data center infrastructure operations, with specialities in CMDB/Configuration management, patch management, monitoring and server migration and standardization.\nThis blog is created in Hugo, an open-source static site generator, and hosted on Microsoft Azure Static Web apps.\nAll content and opinions are my own and do not necessary reflect that of my employer or Microsoft.\n","link":"https://www.thojotech.com/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://www.thojotech.com/archives/","section":"","tags":null,"title":""},{"body":"","link":"https://www.thojotech.com/tags/index/","section":"tags","tags":null,"title":"index"},{"body":"","link":"https://www.thojotech.com/series/","section":"series","tags":null,"title":"Series"}]